{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "overall-spider",
   "metadata": {},
   "source": [
    "# Data Mining - Handin 1 - Clustering \n",
    "Welcome to the handin on clustering algorithms and outlier detection. \n",
    "This handin corresponds to the topics in Week 5--9 in the course.\n",
    "\n",
    "The handin is \n",
    "* done in the chosen handin groups\n",
    "* worth 10% of the final grade\n",
    "\n",
    "For the handin, you will prepare a report in PDF format, by exporting the Jupyter notebook. \n",
    "Please submit\n",
    "1. The jupyter notebook file with your answers\n",
    "2. The PDF obtained by exporting the jupyter notebook\n",
    "\n",
    "**The grading system**: Tasks are assigned a number of points based on the difficulty and time to solve it. The sum of the number of points is 100. For the maximum grade you need to get at least _90 points_. The minimum grade (02 in the Danish scale)\n",
    "requires **at least** 30 points, with at least 8 points from the first three Parts (Part 1,2,3) and 6 points in the last part (Part 4).\n",
    "\n",
    "**The exercise types**: There are four different types of exercises\n",
    "1. <span style='color: green'>**\\[Compute by hand\\]**</span> means that you should provide NO code, but show the main steps to reach the result (not all). \n",
    "2. <span style='color: green'>**\\[Motivate\\]**</span> means to provide a short answer of 1-5 lines indicating the main reasoning, e.g., the PageRank of a complete graph is 1/n in all nodes as all nodes are symmetric and are connected one another.\n",
    "3. <span style='color: green'>**\\[Prove\\]**</span> means to provide a formal argument and NO code. \n",
    "4. <span style='color: green'>**\\[Implement\\]**</span> means to provide an implementation. Unless otherwise specified, you are allowed to use helper functions (e.g., ```np.mean```, ```itertools.combinations```, and so on). **However**, if the task is to implement an algorithm, by no means a call to a library that implements the same algorithm will be deemed as sufficient!\n",
    "\n",
    "**Q&A**\n",
    "\n",
    "Q: If the task is to implement a mean function, may I just call ```np.mean()```? \n",
    "<br>A: No.\n",
    "\n",
    "Q: If the task is to compare the mean of X and Y, may I use ```np.mean()``` to calculate the mean?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: If I have implemented a mean function in a previous task, but I am unsure of its correctness, may I use ```np.mean()``` in following task where mean is used as a helper function? \n",
    "<br>A: Yes.\n",
    "\n",
    "Q: May I use ```np.mean()``` to debug my implementation of mean?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Do I get 0 points for a task if I skip it?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Can I get partial points for a task I did partially correct?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Is it OK to skip a task if I do not need the points from it?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Should I inform a TA if I find an error?\n",
    "<br>A: Yes.\n",
    "\n",
    "Q: Should I ask questions if I am confused?\n",
    "<br>A: Yes.\n",
    "\n",
    "\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "loaded-nicholas",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT TOUCH\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "RANDOM_SEED = 132414\n",
    "## DO NOT TOUCH\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wine = load_wine()\n",
    "df = pd.DataFrame(data=wine.data, columns=wine.feature_names)\n",
    "df['target'] = wine.target\n",
    "color_map = {0:'Blue', 1:'Red', 2:'Green'}\n",
    "toy = df.sample(n=15, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-regression",
   "metadata": {},
   "source": [
    "# Part 1 Intro Excercises\n",
    "\n",
    "## Task 1.1 K-Means and DBScan\n",
    "\n",
    "### Task 1.1.1 (4 points)\n",
    "<span style='color: green'>**\\[Compute by hand\\]**</span> the cluster assignments _for the dataset below_ using k-means and $k = 2$, with initial centroids being (2.2, 0.9) and (2.3,  1.0)\n",
    "\n",
    "<font color='red'>To evaluate (i.e., only to control the correctness and not to solve the exercise) your results you can use **sklearn.cluster.KMeans**.</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "annoying-mapping",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.7855, 2.5444999999999998, 0.5475, 1.2625)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "first = 'ash'\n",
    "last = 'hue'\n",
    "\n",
    "X_kmeans = toy[[first, last]]\n",
    "\n",
    "plt.scatter(X_kmeans[first], X_kmeans[last], alpha=0.8, c=toy['target'].map(color_map))\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "037e5182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ash</th>\n",
       "      <th>hue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>2.19</td>\n",
       "      <td>1.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.45</td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2.48</td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>2.42</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>1.82</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>2.38</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>2.51</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2.50</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.14</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>1.99</td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.21</td>\n",
       "      <td>1.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>2.00</td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>2.17</td>\n",
       "      <td>0.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>2.10</td>\n",
       "      <td>0.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>1.98</td>\n",
       "      <td>0.70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ash   hue\n",
       "117  2.19  1.06\n",
       "6    2.45  1.02\n",
       "18   2.48  1.23\n",
       "91   2.42  1.05\n",
       "110  1.82  0.75\n",
       "129  2.38  0.79\n",
       "39   2.51  0.89\n",
       "114  2.50  0.93\n",
       "1    2.14  1.05\n",
       "116  1.99  0.95\n",
       "29   2.21  1.04\n",
       "119  2.00  0.93\n",
       "125  2.17  0.86\n",
       "154  2.10  0.58\n",
       "118  1.98  0.70"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-matter",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "Calculating the distance from each datapoint to both centroid, and assigning to the closest. Using euclidian distance $$d_p(\\mathbf{x}, \\mathbf{y})=\\sqrt{\\sum_{i=1}^d\\left|x_i-y_i\\right|^2}$$\n",
    "Considering ash as x, and hue as y.\n",
    "$c_1=(2.2,0.9), c_2=(2.3,1.0)$\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca901b75",
   "metadata": {},
   "source": [
    "datapoint 117:  \n",
    " $d_{c_1}(x,y)=\\sqrt{(2.19-2.2)^2+(1.06-0.9)^2}= 0.160$  \n",
    "$d_{c_2}(x,y)=\\sqrt{(2.19-2.3)^2+(1.06-1.0)^2}=0.125$  \n",
    "Therefor we assign datapoint 117 to $c_2$, since it is closer to that. If we do this for all 15 points of the toy-dataset, we get the following clustering.  \n",
    "$c_2$ clustering: 117, 6, 18, 91, 129, 39, 114, 29  \n",
    "$c_1$ clustering: 110, 1, 116, 119, 125, 154, 118\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-somerset",
   "metadata": {},
   "source": [
    "### Task 1.1.2 (3 points)\n",
    "<span style='color: green'>**\\[Compute by hand\\]**</span> <br>\n",
    "A) Using examples, show why the k-means algorithm may not find the global optimum. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-vertex",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "\n",
    "The result depends on the initialization centers. Where the initial centers may end in a local optimum.  \n",
    "\n",
    "Example in 1d.  \n",
    "\n",
    "Having a dataset of $\\{1,2,3,9\\}$ where the first three are one label, and last is another label, with two clusters. If centers are $\\{2\\}$, $\\{8\\}$ we end in a global optimum, but if centers are $\\{1.5\\}$, $\\{3.5\\}$ we end in a local optimum.\n",
    "\n",
    "\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a686bab0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "israeli-chicago",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span> <br>\n",
    "B) K-means vs K-medoids: Which performs better when the dataset contains outliers, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-office",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statutory-management",
   "metadata": {},
   "source": [
    "### Task 1.1.3 (4 points)\n",
    "<span style='color: green'>**\\[Compute by hand\\]**</span> the dendrogram for the dataset of Task 1.1.1. using **complete-link**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-template",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-benjamin",
   "metadata": {},
   "source": [
    "### Task 1.1.4 (3 points)\n",
    "A) <span style='color: green'>**\\[Compute by hand\\]**</span> the density-based clustering DBSCAN for the dataset of Task 1.1.1 using $\\epsilon=0.2$ and $MinPts=3$. Present at least 2 iterations of the algorithm<br> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-emperor",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-briefs",
   "metadata": {},
   "source": [
    "\n",
    "B) <span style='color: green'>**\\[Motivate\\]**</span> the difference between the clusters obtained with DBSCAN and those obtained with KMeans in Task 1.1.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-programming",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spanish-fields",
   "metadata": {},
   "source": [
    "## Task 1.2 Elliptic data set (2 points)\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> <br> \n",
    "After looking at the dataset _below_, you want to detect the red outlier point, assuming you know that it is an outlier. \n",
    "\n",
    "Which approach would be the most obvious to find the red outlier? Please (1) check the box and (2) motivate your answer below:\n",
    "- [ ] Distance based approach (with parameteres $\\pi=0.5$, $\\epsilon=2$ and euclidean distance)\n",
    "- [ ] Angle based approach\n",
    "- [ ] Depth based approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "suspended-legend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D_new = np.array([[1.5, 1.0], # Red \n",
    "                [1., 1.0],\n",
    "                [0.5, 0.5],\n",
    "                [1, 0.5],\n",
    "                [0.5, 1],\n",
    "                [0.75, 0.75]\n",
    "                 ])\n",
    "\n",
    "plt.scatter(D_new[:, 0], D_new[:, 1], alpha=0.8, c = ['red' if i == 0 else 'blue' for i in range(len(D_new))])\n",
    "plt.axis([0, 2, 0, 2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removable-tuesday",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-extraction",
   "metadata": {},
   "source": [
    "## Task 1.3 Theoretical questions\n",
    "### Task 1.3.1 Statistical Analysis (3 points)\n",
    "<span style='color: green'>**\\[Prove\\]**</span> 1. Prove that the Euclidean distance is a pseudometric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-column",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dbd657",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Prove\\]**</span> 2. Prove that the sample mean is an unbiased estimator of the population mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412c9cdc",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99b431d",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Prove\\]**</span> 3. Prove that the sample variance is an asymptotically unbiased estimator of the population variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7831962c",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515bf551",
   "metadata": {},
   "source": [
    "### Task 1.3.2 Kernel trick (7 points)\n",
    "<span style='color: green'>**\\[Motivation\\]**</span> A) What is a positive-definite kernel $K(x,x')$ of two vectors $x,x'\\in\\mathbb{R}^n$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0696e0",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8249d778",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span> B) Please explain briefly what is the kernel trick method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d0ba08",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fe260d",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Prove\\]**</span> C)\n",
    "Let two positive-definite kernels $K_1(x,x')$ and $K_2(x,x')$.  <br> Show that functions $K_1(x,x')+K_2(x,x')$ and $K_1(x,x')K_2(x,x')$ are also positive-definite kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30b07b7",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef94954d",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Prove\\]**</span> D) Prove that $K(x,x')=e^{2 ln(x^{\\top}x')-(x-x')^{\\top}(x-x')}$ is a positive-definite kernel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f812c",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metropolitan-artwork",
   "metadata": {},
   "source": [
    "# Part 2 Exploratory data analysis\n",
    "In this section, you will perform preliminary analysis on your data. These preliminary analysis are useful to understand how the data behaves, before running complex algorithms.<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guilty-vegetable",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = df\n",
    "data_np = toy.to_numpy()\n",
    "headers = ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium','total_phenols', 'flavanoids', 'nonflavanoid_phenols',\n",
    "       'proanthocyanins', 'color_intensity', 'hue','od280/od315_of_diluted_wines', 'proline', 'target']\n",
    "X = data_np[:,3:]\n",
    "y = data_np[:,11]\n",
    "rows, cols = np.shape(X)\n",
    "toy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-ottawa",
   "metadata": {},
   "source": [
    "## Task 2.1 Correlation matrix\n",
    "### Task 2.1.1 (5 points)\n",
    "A) <span style='color: green'>**\\[Implement\\]**</span> in the code-box below the **correlation matrix** (not covariance matrix) among all the attributes. <br>\n",
    "<font color='red'>To CHECK your results you can use **numpy.corrcoef**.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-seeking",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_matrix(X):\n",
    "    corr = None\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE \n",
    "    return corr\n",
    "    \n",
    "X = data_np\n",
    "Corr = correlation_matrix(X)\n",
    "plt.matshow(Corr)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suburban-volume",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "B) By observing the  **correlation matrix** in A), which pair(s) of different features has the highest correlation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "median-silver",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "higher-senator",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "C) What does it mean that two features are highly correlated? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "creative-breath",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-springfield",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "D) Based on the features of the data in Part 2 and your answer in C), did you expect the observation of B)? <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "configured-weekly",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-content",
   "metadata": {},
   "source": [
    "### Task 2.1.2 (2 points)\n",
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "\n",
    "Plot the correlation matrix running the code below. (You may need to zoom on it)\n",
    "What is the relationship between the correlation matrix and the covariance matrix? (1) Check the correct box below and (2) motivate your answer.\n",
    "\n",
    "- [ ] The correlation matrix contains the unnormalized covariance values\n",
    "- [ ] The correlation matrix contains the normalized covariance values\n",
    "- [ ] The covariance matrix contains the variance of the correlation\n",
    "\n",
    "<font color='red'>Do NOT just choose an answer. Please clarify WHY this is the correct answer.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "productive-newark",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_plot = df.drop(['target'],axis=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(df_to_plot.corr(),annot=True,linewidths=1, cmap=\"YlGnBu\", annot_kws={\"fontsize\":10}, vmax=1, ax=ax)\n",
    "plt.title('Correlation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dimensional-behalf",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-shelter",
   "metadata": {},
   "source": [
    "### Task 2.1.3 (3 points)\n",
    "\n",
    "In this task, we reason about the covariance matrices.\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> code for normalizing the features of the wine dataset using (1) standard score normalization and (2) range normalization. Finally, (3) plot the **covariance** matrices for\n",
    "1. The unnormalized data\n",
    "2. The [standard score normalized features](https://en.wikipedia.org/wiki/Standard_score)\n",
    "3. The range (min-max) normalized features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broadband-reverse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = data_np\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-decision",
   "metadata": {},
   "source": [
    "### Task 2.1.4 (2 points)\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> how the covariance matrix changes with different normalization schemes and reason on why such behaviour appears.\n",
    "You should notice some differences. (1) Check the correct box below and (2) motivate your answer.\n",
    "\n",
    "\n",
    "\n",
    "- [ ] Range normalization preserves the variance. Therefore, features are directly comparable within the matrix.\n",
    "- [ ] Standard score normalization preserves the variance. Therefore, features are directly comparable within the matrix.\n",
    "- [ ] Both methods normalize in such a way, that it makes sense to compare the different covariance values to each other within the matrix. \n",
    "- [ ] None of the methods normalize in such a way that it makes sense to compare the different covariance values to each other.\n",
    "\n",
    "<font color='red'>Do NOT just choose an answer. Please clarify WHY this is the correct answer.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-adapter",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-entry",
   "metadata": {},
   "source": [
    "## Task 2.2 Normal distribution\n",
    "### Task 2.2.1 (4 points)\n",
    "Sometimes it is convenient to know whether a variable is close to a normal distribution.\n",
    "\n",
    "<span style='color: green'>**\\[Implement\\]**</span> a method norm_dist that: <br>\n",
    "    \n",
    "1) **Inputs**: \n",
    "    * the number of buckets $b$ \n",
    "    * a vector $x$ of values \n",
    "2) First, compute the histogram of a Gaussian variable with mean $\\mu$ corresponding to the sample mean of $x$ and $\\sigma^2$ corresponding to the sample variance of $x$. Second, calculate the histogram of $x$ using $b$ buckets. \n",
    "3) **Output**: the sum of the absolute differences of the buckets between the two histograms computed in 2). The sum of the differences is computed as \n",
    "$$\\sum_{i=1}^b |H_X(i) - H_{\\mathcal{N}}(i)|$$ \n",
    "where $H_X(i)$ is the i-th bucket of the histogram of $x$ and $H_\\mathcal{N}(i)$ is the i-th bucket of the hisotgram obtained from the normal distribution $\\mathcal{N}(\\mu,\\sigma^2)$. \n",
    "\n",
    "<font color='red'>You can use the norm function from Scipy to get the normal distribution to subtract from.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "willing-bobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "## Our data comes from the variable X\n",
    "X = data_np\n",
    "def norm_dist(x, b): \n",
    "    dist = 0\n",
    "    ### YOUR CODE HERE\n",
    "    \n",
    "    \n",
    "    ### YOUR CODE HERE\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coated-mission",
   "metadata": {},
   "source": [
    "### Task 2.2.2 (6 point)\n",
    "A) <span style='color: green'>**\\[Motivate\\]**</span> which drawbacks the method in Task 2.2.1 has. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afraid-sharp",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-blowing",
   "metadata": {},
   "source": [
    "B) <span style='color: green'>**\\[Motivate\\]**</span> whether the method in Task 2.2.1  is robust to outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-blade",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-crown",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Implement\\]**</span><br>\n",
    "C) Run your code on each columns of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-ceramic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-female",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "D) What is the column with the largest distance? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-workplace",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-delay",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "E) Do the features follow a normal distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instant-match",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-cyprus",
   "metadata": {},
   "source": [
    "### Task 2.2.3 (2 points)\n",
    "\n",
    "Now look at the method below. This is called a Quantile-Quantile [Q-Q plot](https://en.wikipedia.org/wiki/Q%E2%80%93Q_plot). \n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> why this method is more robust than the one we proposed in Task 2.2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "environmental-battery",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "from matplotlib import gridspec\n",
    "\n",
    "plt.tight_layout()\n",
    "_, n = X.shape\n",
    "print(headers, X.shape)\n",
    "fig = plt.figure(constrained_layout=True, figsize=(8, 50))\n",
    "spec = gridspec.GridSpec(ncols=2, nrows=(n-1), figure=fig)\n",
    "for i in np.arange(2,n):\n",
    "    x = toy[headers[i]]\n",
    "    r = i-1\n",
    "    qq = fig.add_subplot(spec[r, 1]) \n",
    "    stats.probplot(x, plot=qq)\n",
    "    h = fig.add_subplot(spec[r, 0])\n",
    "    h.set_title(headers[i])\n",
    "    h.hist(x, bins = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-harbor",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "collaborative-kinase",
   "metadata": {},
   "source": [
    "# Part 3 Cluster Analysis\n",
    "In this section, you will perform cluster analysis of the dataset in Part 2 and modify clustering algorithms to achieve better results. \n",
    "\n",
    "## Task 3.1\n",
    "\n",
    "### Task 3.1.1 (6 points)\n",
    "A)  <span style='color: green'>**\\[Implement\\]**</span> and plot the **silhouette coefficient** to detect the number of clusters $k$. \n",
    "\n",
    "<font color='red'>You can use the KMeans implementation from scikit-learn.</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25da57f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "toy = df\n",
    "first = \"alcohol\"\n",
    "second = \"flavanoids\"\n",
    "X = toy[[first, second]].to_numpy()\n",
    "y = toy['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unknown-renaissance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = toy[[first, second]].to_numpy()\n",
    "### YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-bubble",
   "metadata": {},
   "source": [
    "B) <span style='color: green'>**\\[Motivate\\]**</span> your choice of clusters $k$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "traditional-pharmacy",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-mortgage",
   "metadata": {},
   "source": [
    "C) <span style='color: green'>**\\[Implement\\]**</span><br>\n",
    "Run k-means on the dataset X, with the number of clusters detected in the previous exercise.\n",
    "\n",
    "<font color='red'>You can use the KMeans implementation from scikit-learn.</font> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-shoot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "X = toy[[first, second]].to_numpy()\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337f7c21-17b1-4571-a03e-a84e2ca48ca1",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span> Did you find better clusters? Are they more separated? Why is it a good/bad idea to use the Silhouette coefficient? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319eec41-78f2-4195-bd32-3d8ff42e432a",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-power",
   "metadata": {},
   "source": [
    "### Task 3.1.2 (3 points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> Kernel K-means and the Gaussian Kernel. \n",
    "\n",
    "The Gaussian kernel is defined as in the following equation:\n",
    "\n",
    "$$\n",
    "K\\left(\\mathbf{x}_{i}, \\mathbf{x}_{j}\\right)=\\exp \\left(-\\frac{\\left\\|\\mathbf{x}_{i}-\\mathbf{x}_{j}\\right\\|^{2}}{2 \\sigma^{2}}\\right)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "previous-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['malic_acid', \"magnesium\"]].to_numpy()\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)\n",
    "X_scaled = StandardScaler().fit(X_norm).transform(X_norm)\n",
    "\n",
    "y = df[['target']].to_numpy()\n",
    "\n",
    "def gaussian_kernel(x,y, sigma=0.2): \n",
    "    k = 0 \n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "    return k\n",
    "\n",
    "\n",
    "def kernel_kmeans(X, n_clusters, kernel=gaussian_kernel, iters=100, error=0):\n",
    "    # For simplicity use 'init' as initial points for your algorithm\n",
    "    kmeans = KMeans(n_clusters=n_clusters, init=\"k-means++\",max_iter=1).fit(X)\n",
    "    init = kmeans.cluster_centers_\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "        \n",
    "    ### YOUR CODE HERE\n",
    "    return clusters\n",
    "\n",
    "# clusters = kernel_kmeans(X_scaled, 3)\n",
    "# plt.scatter(X[:, 0], X[:, 1], alpha=0.8, c=clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e186498",
   "metadata": {},
   "source": [
    "### Task 3.1.3 (3 points)\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> Run both kmeans and kernel K-means on the data. \n",
    "Which clustering do you think is better and explain why do you think so? Which one most resemble the ground truth labeling? Under which condition is Kernel K-means with Gaussian Kernel better than K-means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b39cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_clustering = KMeans(n_clusters=3).fit(X_scaled).labels_\n",
    "gaussian_clustering = kernel_kmeans(X_scaled, 3, kernel=gaussian_kernel)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12,4))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], alpha=0.8, c=kmeans_clustering)\n",
    "axes[0].set_title(\"K-means\")\n",
    "\n",
    "axes[1].scatter(X[:, 0], X[:, 1], alpha=0.8, c=gaussian_clustering)\n",
    "axes[1].set_title(\"Gaussian K-means\")\n",
    "\n",
    "axes[2].scatter(X[:, 0], X[:, 1], alpha=0.8, c=y)\n",
    "axes[2].set_title(\"Ground Truth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f13f2f2",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3a634a",
   "metadata": {},
   "source": [
    "### Task 3.1.4 (4 points)\n",
    "<span style='color: green'>**\\[Motivate\\]**</span> Running the code below, draw two example datasets such that:<br>\n",
    "a). K-means produces clustering better than the one obtained by Gaussian K-means.<br>\n",
    "b). Gaussian K-means produces clustering better than the one obtained by Vanilla K-means.\n",
    "\n",
    "Explain why do you think one clustering is better than the other and give the reason why K-means and Gaussian K-means behave this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c0e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can use this tool to generate the data\n",
    "from drawdata import ScatterWidget\n",
    "widget = ScatterWidget()\n",
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33075897",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Run this code to print data from the tool above\n",
    "if len(widget.data) != 0:\n",
    "    data = widget.data_as_pandas[['x', 'y']][:400].to_numpy()\n",
    "    print(np.array2string(data, precision=0, separator=',', ).replace('\\n', ''))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc865c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### You can use this code for plotting. It's just a template you can modify it if you want to.\n",
    "\n",
    "# Save your example dataset in the array below\n",
    "X = np.array([])\n",
    "\n",
    "# Use the line below for debugging, so that you don't have to copy paste data every time\n",
    "# X = widget.data_as_pandas[['x', 'y']].to_numpy()[:400]\n",
    "\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)\n",
    "X_scaled = StandardScaler().fit(X_norm).transform(X_norm)\n",
    "\n",
    "n_clusters = 2\n",
    "kmeans_clustering = KMeans(n_clusters=n_clusters, random_state=2137).fit(X_scaled).labels_\n",
    "gaussian_clustering = kernel_kmeans(X_scaled, n_clusters, kernel=gaussian_kernel)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], alpha=0.8, c=kmeans_clustering)\n",
    "axes[0].set_title(f\"K-means\")\n",
    "\n",
    "axes[1].scatter(X[:, 0], X[:, 1], alpha=0.8, c=gaussian_clustering)\n",
    "axes[1].set_title(f\"Gaussian K-means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f036789",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "transparent-junction",
   "metadata": {},
   "source": [
    "\n",
    "## Task 3.2 Clustering quality\n",
    "\n",
    "### Task 3.2.1 (2 points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> **Normalized Mutual Information (NNI)** as a measure for clustering quality.\n",
    "\n",
    "\n",
    "**Hint**: First implement **Entropy** and then **Normalized Mutual Information**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "opposed-research",
   "metadata": {},
   "outputs": [],
   "source": [
    "### C is the clustering obtained by an algorithm and T is the ground truth cluster assignments.\n",
    "\n",
    "def entropy(C):\n",
    "    ### IMPLEMENT\n",
    "    return None\n",
    "\n",
    "\n",
    "def NMI(C, T):\n",
    "    ### IMPLEMENT\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d403e7",
   "metadata": {},
   "source": [
    "Run the code below to measure the quality of clustering obtained by k-means in task 3.1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "93345c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Mutual Information: None\n"
     ]
    }
   ],
   "source": [
    "X = df[['malic_acid', \"magnesium\"]].to_numpy()\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)\n",
    "y = df[['target']].to_numpy()\n",
    "\n",
    "\n",
    "T = y # Ground-truth clusters\n",
    "C = KMeans(n_clusters=3).fit_predict(X_norm)# Clusters obtained by k-means\n",
    "\n",
    "\n",
    "print(f'Normalized Mutual Information: {NMI(C, T)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e941bfce",
   "metadata": {},
   "source": [
    "### Task 3.2.2 (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-soundtrack",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "A) Reason about the measure, is the measure influenced by the size of the clusters?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-convert",
   "metadata": {},
   "source": [
    "******************* \n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-police",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "B) What does the measure capture? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "understanding-settle",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0834480",
   "metadata": {},
   "source": [
    "### Task 3.2.3 (4 points)\n",
    "A) <span style='color: green'>**\\[Implement\\]**</span> functions computing **Purity** and **F-measure** of a clustering and a ground truth labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "0dec601a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### C is the clustering obtained by an algorithm and T is the ground truth cluster assignments.\n",
    "\n",
    "\n",
    "def purity(C, T):\n",
    "    ### IMPLEMENT\n",
    "    return None\n",
    "\n",
    "\n",
    "def f_measure(C, T):\n",
    "    ### IMPLEMENT\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5127394",
   "metadata": {},
   "source": [
    "Run the code below to measure the quality of clustering obtained by k-means in task 3.1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31d267d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['malic_acid', \"magnesium\"]].to_numpy()\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)\n",
    "y = df[['target']].to_numpy()\n",
    "\n",
    "\n",
    "T = y # Ground-truth clusters\n",
    "C = KMeans(n_clusters=3).fit_predict(X_norm)# Clusters obtained by k-means\n",
    "\n",
    "\n",
    "print(f'Purity: {purity(C, T)}')\n",
    "print(f'F-measure: {f_measure(C, T)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b31d0cb-8a86-4936-9538-75dc77bae7ca",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "B) Reason about the differences in the measures, is one more affected by some the characteristics of the clusters (e.g. size, density, radius)? If so, why? What are the drawbacks and advantages of each measure? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36e1919-7d23-4115-b430-3083acf58f7a",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-sector",
   "metadata": {},
   "source": [
    "### Task 3.2.4 (2 points)\n",
    "\n",
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "\n",
    "(1) Check the correct box (or boxes) below and (2) motivate your answers.\n",
    "\n",
    "- [ ] Conditional Entropy is preferable over Entropy because it uses all the points.\n",
    "- [ ] F-measure is preferable over Purity because it is less computational demanding.\n",
    "- [ ] Contingency table is always a square matrix.\n",
    "- [ ] As number of clusters increases Purity tends to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepted-influence",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "generic-state",
   "metadata": {},
   "source": [
    "## Task 3.3 Gaussian Mixtures and the EM-Algorithm\n",
    "### Task 3.3.1 (4 point)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> the EM-algorithm for the Gaussian Mixture Model.\n",
    "<br> You can consult [DMA] Section 13.3.2, for a description of how the algorithm works in this particular setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sensitive-weekly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities.gmm import GMM\n",
    "class MyGMM(GMM):\n",
    "    def initialize_parameters(self, X):\n",
    "        \"\"\"\n",
    "            This function should utilize information from the data to initialize\n",
    "            the parameters of the model.\n",
    "            In particular, it should compute initial values for mu, Sigma, and pi.\n",
    "            \n",
    "            The function corresponds to line 2-4 in Algorithm 13.3 in [DMA, p. 349]\n",
    "            Note, that K can be retrieved as `self.K`.\n",
    "\n",
    "            Args:\n",
    "                X (matrix, [n, d]): Data to be used for initialization.\n",
    "\n",
    "            Returns:\n",
    "                Tuple (mu, Sigma, pi), \n",
    "                    mu has size        [K, d]\n",
    "                    Sigma has size     [K, d, d]\n",
    "                    pi has size        [K]\n",
    "        \"\"\"\n",
    "        # TODO: what should the values be for initializing mu, Sigma and pi\n",
    "        return mu, Sigma, pi\n",
    "\n",
    "\n",
    "    def posterior(self, X):\n",
    "        \"\"\"\n",
    "            The E-step of the EM algorithm. \n",
    "            Returns the posterior probability p(Y|X)\n",
    "\n",
    "            This function corresponds to line 8 in Algorithm 13.3 in [DMA, p. 349]\n",
    "            Note, that mean and covariance matrices can be accessed by `self.mu` and `self.Sigma`, respectively.\n",
    "            \n",
    "            Args:\n",
    "                X (matrix, [n,  d]): Data to compute posterior for.\n",
    "\n",
    "            Returns:\n",
    "                Matrix of size        [n, K]\n",
    "        \"\"\"\n",
    "        # TODO: what is the posterior probability?\n",
    "        \n",
    "        return posterior\n",
    "        \n",
    "\n",
    "    def m_step(self, X, P):\n",
    "        \"\"\"\n",
    "            Update the estimates of mu, Sigma, and pi, given the data `X` and the current\n",
    "            posterior probabilities `P`.\n",
    "\n",
    "            This function corresponds to line 10-12 in Algorithm 13.3 and Eqn. (13.11-13) in [DMA, p. 349].\n",
    "            \n",
    "            Args:\n",
    "                X (matrix, [n, d]): Data matrix\n",
    "                P (matrix, [n, K]): The posterior probabilities for the n samples.\n",
    "\n",
    "            Returns:\n",
    "                Tuple (mu, Sigma, pi), \n",
    "                    mu has size        [K, d]\n",
    "                    Sigma has size    [K, d, d]\n",
    "                    pi has size        [K]\n",
    "        \"\"\"\n",
    "        # TODO: what is the values of mu, Sigma, and pi that maximizes the expectation given the posterior?\n",
    "        return  mu_hat, Si_hat, pi_hat\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "political-wireless",
   "metadata": {},
   "source": [
    "### Task 3.3.2 (4 points)\n",
    "\n",
    "Run both k-means and your EM-algorithm for GaussianMixtures<br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b5129",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['malic_acid', \"magnesium\"]].to_numpy()\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)\n",
    "\n",
    "n_clusters = 3\n",
    "kmeans_clustering = KMeans(n_clusters=n_clusters).fit_predict(X_norm)\n",
    "\n",
    "model = MyGMM(K=n_clusters)\n",
    "model.fit(X_norm)\n",
    "gmm_clustering = model.predict(X_norm)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
    "axes[0].scatter(X[:, 0], X[:, 1], alpha=0.8, c=kmeans_clustering)\n",
    "axes[0].set_title(f\"K-means\")\n",
    "\n",
    "axes[1].scatter(X[:, 0], X[:, 1], alpha=0.8, c=gmm_clustering)\n",
    "axes[1].set_title(f\"EM Gaussian Mixtures\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd0ed97",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "A) Can you see the substantial difference between those two clusterings? Explain it.\n",
    "\n",
    "(*if you don't see any difference try running the algorithm again*)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd55a97",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d60e5df",
   "metadata": {},
   "source": [
    "<span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "B) What are the advantages and disadvantages of this approach?<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8058f8",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "published-culture",
   "metadata": {},
   "source": [
    "# Part 4 Outlier detection\n",
    "In this exercise we will work with outlier detection techniques and analyze their performance on the small dataset. Before starting the exercise, run the code below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "explicit-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['malic_acid', \"magnesium\"]].to_numpy()\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "royal-mumbai",
   "metadata": {},
   "source": [
    "## Task 4.1 (DBoutliers)\n",
    "We will now compare two outlier detection techniques.\n",
    "### Task 4.1.1 (2 points)\n",
    "<span style='color: green'>**\\[Implement\\]**</span> a simple distance-based outlier detector. This is the distance-based outlier detection from the lectures, where a point is considered an outlier if at most a fraction $pi$ of the other points have a distance less of than $eps$ to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "grave-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DBOutliers(X, eps, pi): \n",
    "    outliers = None\n",
    "    ### YOUR CODE STARTS HERE\n",
    "    \n",
    "    \n",
    "    ### YOUR ENDS CODE HERE\n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relative-somewhere",
   "metadata": {},
   "source": [
    "### Task 4.1.2 (4 points)\n",
    "A) <span style='color: green'>**\\[Implement\\]**</span>\n",
    "DBOutliers requires tuning the parameters eps, pi. Run the code from Task 4.1.1 with different choices of eps, pi \n",
    "\n",
    "**Note** that the data is normalized. Choose two ranges with **at least** 4 values each.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollywood-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developmental-simulation",
   "metadata": {},
   "source": [
    "B) <span style='color: green'>**\\[Motivate\\]**</span><br>\n",
    "\n",
    "**Present** the results  and **discuss** how the results vary with respect to (1) eps and (2) pi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satellite-stanford",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9fb942",
   "metadata": {},
   "source": [
    "## Task 4.2 Isolation Forest (8 points)\n",
    "\n",
    "In this section you will recreate implementation of the Isolation Forest from the original paper: https://ieeexplore.ieee.org/abstract/document/4781136.\n",
    "\n",
    "<span style='color: red'>**Note**: To access the PDF, you must be connected to the university's Wi-Fi or VPN.</span><br>\n",
    "\n",
    "\n",
    "A) <span style='color: green'>**\\[Implement\\]**</span> Fill missing parts in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f52c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c(n):\n",
    "    \"\"\"\n",
    "    Defined by the equation (1) in the paper.\\\\\n",
    "    Computes the expected path length for a given tree size based on the average depth of\n",
    "    a randomly generated binary search tree.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n (int): The number of data points in the node.\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    float: The expected path length.\n",
    "    \"\"\"\n",
    "    if n <= 1:\n",
    "        return 0\n",
    "    ### YOUR CODE STARTS HERE\n",
    "    \n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    \n",
    "\n",
    "\n",
    "class IsolationTree:\n",
    "    def __init__(self):\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.split_feature = None\n",
    "        self.split_value = None\n",
    "        self.size = 0\n",
    "\n",
    "\n",
    "    def fit(self, X, e, l):\n",
    "        \"\"\"\n",
    "        Defined by the Algorithm 2 in the paper.\\\\\n",
    "        Trains the tree on data X.\\\\\n",
    "        It's already implemented for you. Do not modify it.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: array-like, shape (n_samples, n_features)\n",
    "        e: current tree height\n",
    "        l: height limit\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        IsolationTree: itself\n",
    "        \"\"\"\n",
    "        self.size = len(X)\n",
    "        \n",
    "        if e >= l or self.size <= 1:\n",
    "            return self\n",
    "        \n",
    "        # Choose a random feature and split value\n",
    "        self.split_feature = np.random.randint(X.shape[1])\n",
    "        min_val, max_val = X[:, self.split_feature].min(), X[:, self.split_feature].max()\n",
    "        \n",
    "        if min_val == max_val:\n",
    "            return self\n",
    "        \n",
    "        self.split_value = np.random.uniform(min_val, max_val)\n",
    "        \n",
    "        left_mask = X[:, self.split_feature] < self.split_value\n",
    "        X_left, X_right = X[left_mask], X[~left_mask]\n",
    "        \n",
    "        self.left = IsolationTree().fit(X_left, e + 1, l)\n",
    "        self.right = IsolationTree().fit(X_right, e + 1, l)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "\n",
    "    def is_external_node(self):\n",
    "        \"\"\"\n",
    "        Checks whether the tree is an external node.\\\\\n",
    "        For the definition of external node ctr+F 'external-node' in the paper.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        Boolean: True is the tree is an external node, otherwise, False.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE\n",
    "    \n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "        \n",
    "    \n",
    "\n",
    "    def path_length(self, x, e=0):\n",
    "        \"\"\"\n",
    "        Defined by the Algorithm 3 in the paper.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: array-like, shape (n_features,)\n",
    "        e: current tree height\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        int: path length of x\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "class IsolationForest:\n",
    "    def __init__(self, n_trees, subsample_size):\n",
    "        self.n_trees = n_trees\n",
    "        self.subsample_size = subsample_size\n",
    "        self.height_limit = np.ceil(np.log2(self.subsample_size))\n",
    "        self.trees = []\n",
    "\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Trains the forest on data X.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: array-like, shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        self.trees = []\n",
    "        for _ in range(self.n_trees):\n",
    "            sample = X[np.random.choice(X.shape[0], self.subsample_size, replace=False)]\n",
    "            tree = IsolationTree().fit(sample, 0, self.height_limit)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "\n",
    "    def anomaly_score(self, x):\n",
    "        \"\"\"\n",
    "        Defined by equation (2) in the paper.\\\\\n",
    "        Computes the anomaly score of instance x.\n",
    "        \n",
    "        Note:\n",
    "        -----------\n",
    "        Keep in mind that variable 'n' in the equation in the paper does not represent the size of X but rather the size of a tree i.e. sub-sampling size.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        x: array-like, shape (n_features,)\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        float: the anomaly score of instance x\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "\n",
    "\n",
    "    def identify_outliers(self, X, threshold=0.5):\n",
    "        \"\"\"\n",
    "        Identifies outliers in the X based on an anomaly score.\n",
    "\n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "        threshold: the cutoff value for determining whether a sample is an outlier.\n",
    "\n",
    "        Returns:\n",
    "        -----------\n",
    "        outliers : array-like, shape (n_samples,)\\\\\n",
    "        A boolean array where each element corresponds to a sample in X.\\\\\n",
    "        True indicates that the anomaly score of the sample is greater than the threshold.\n",
    "        \"\"\"\n",
    "        ### YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c638507",
   "metadata": {},
   "source": [
    "B). <span style='color: green'>**\\[Implement\\]**</span> Create an Isolation Forest with $n\\_trees = 100$, $subsample\\_size = 50$ and train it on $X$. Then, use the Isolation Forest to find outliers in $X$ and plot the result. You might need to fine-tune parameter $threshold$ so that the result is meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b39e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['malic_acid', \"magnesium\"]].to_numpy()\n",
    "X_norm = (X - X.min(0)) / X.ptp(0)\n",
    "\n",
    "### YOUR CODE STARTS HERE\n",
    "\n",
    "\n",
    "### YOUR CODE ENDS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247b2200-c009-425d-8ea5-b5095e3ece7e",
   "metadata": {},
   "source": [
    "C) <span style='color: green'>**\\[Motivate\\]**</span> Present the results and discuss what you found and what is the impact of $threshold$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed7a55c-3fb1-4579-b6f0-2f1178829e42",
   "metadata": {},
   "source": [
    "*******************\n",
    "**YOUR ANSWER HERE**\n",
    "******************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db1201f-54cb-46de-afdd-4bf2f4338264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
